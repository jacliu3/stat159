---
title: "Linear Regression Report"
author: "Jacqueline Liu"
date: "October 15, 2016"
output: pdf_document
header-includes:
- \usepackage{float}
---
```{r echo=FALSE}
knitr::opts_chunk$set(fig.width=4, fig.height=2.7, fig.aligh='center', echo=FALSE, warning=FALSE, message=FALSE)
```
```{r echo=FALSE}
library(png)
library(grid)
library(xtable)
options(xtable.comment = FALSE,
        xtable.table.placement="H")
path <- getwd()
load(paste0(path, "/../data/regression.Rdata"))
load(paste0(path, "/../data/correlation-matrix.Rdata"))
source(paste0(path, "/../code/functions/regression-functions.R"))
```

### Abstract:
Linear regression is a simple, yet powerful method of modeling and supervised learning. Many real-world phenomena can be represented as a linear model, or with other models that employ linear regression. The well-known and well-referenced text **An Introduction to Statistical Learning** covers this technique in chapter 3, sections 1 and 2, which cover simple and multiple linear regression. Here we will reproduce their results to highlight key steps and ideas.


### Introduction:
There is a wide variety of applications for linear regression; it can be used for prediction of response to a medical treatment, forecasting prices of fruits a month from now, or determining whether or not sleep and grades are correlated. Ultimately, the goal is to describe some trend or relationship in a dataset. Once the appropriate model is found, predictions can be made for certain variables if given the values of others and the strength of relationships can be assessed. Here, we'll be looking at a commercial setting and determining the relationship between advertising spending and sales of a given product. Specifically, we want to predict sales given the advertising budget for three different media outlets. 


### Data:
The `Advertising` dataset was 'collected' from 200 different markets. It records the advertising budgets (in thousands of dollars) for `TV`, `Radio`, and `Newspaper` as well as the number of `Sales` (in thounds of units) for each of the markets. Because `Sales` is what we're ultimately trying to predict, it is our response variable while the other three columns will be our features. 

### Methodology:
We're using linear models, which can be written as:
$$ Y = \beta_0 + \beta_1 X$$
Given X and Y, the coefficients $\beta_0$ and $\beta_1$ are chosen to minimize the sum of squared errors:
$$ SSE = \sum (Y - \hat{Y})^2 $$
giving our regression line the name "least squares regression". There are other loss metrics we could use (weighted errors, least absolute deviation, L1-norm penalty) but least squares is one of the most widedly used. We can regress `Sales` on `TV`, then on `Radio`, and `Newspaper` to find three different linear models. We will also regress `Sales` on all three variables at once to better understand their high-dimensional relationship, following the model
$$ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3$$

\newpage
### Results:

To understand the linear relationship between the features, here is the correlation matrix:
```{r, results='asis'}
xtable(cm, caption="Correlation Matrix")
```

and here is the scatterplot matrix:

```{r fig.width=5, fig.height=5}
sm.img <- readPNG(paste0(path, "/../images/scatterplot-matrix.png"))
grid.raster(sm.img)
```

The regression coefficients for the simple linear regression models are:

```{r, results='asis'}
xtable(tv.sum$coefficients, caption="Sales onto TV")

xtable(radio.sum$coefficients, caption="Sales onto Radio")

xtable(news.sum$coefficients, caption="Sales onto Newspaper")
```

and their quality indices:

```{r, results='asis'}
xtable(data.frame(Quantity=c('RSS', 'R2', 'F-stat'), 
                  Value=c(residual_std_error(tv.fit), r_squared(tv.fit), f_statistic(tv.fit))), 
       caption="TV Regression Quality Indices")

xtable(data.frame(Quantity=c('RSS', 'R2', 'F-stat'), 
                  Value=c(residual_std_error(radio.fit), r_squared(radio.fit), f_statistic(radio.fit))), 
       caption="Radio Regression Quality Indices")

xtable(data.frame(Quantity=c('RSS', 'R2', 'F-stat'), 
                  Value=c(residual_std_error(news.fit), r_squared(news.fit), f_statistic(news.fit))), 
       caption="Newspaper Regression Quality Indices")
```

(NOTE: the p-values are not exactly 0, but so small that for all intents and purposes they basically are).
For the multiple linear regression, the coefficients are:

The multiple regression model gives rise to the following coefficients and quality indices:

```{r, results='asis'}
xtable(mult.sum$coefficients, caption="Sales onto TV, Radio, and News")

xtable(data.frame(Quantity=c('RSS', 'R2', 'F-stat'), 
                  Value=c(residual_std_error(mult.fit), r_squared(mult.fit), f_statistic(mult.fit))), 
       caption="Multiple Regression Quality Indices")
```


To visualize the data and the proposed simple linear models, here are scatterplots with the calculated regression lines:

```{r}
tv.img <- readPNG(paste0(path, "/../images/scatterplot-tv-sales.png"))
grid.raster(tv.img)
```

```{r}
radio.img <- readPNG(paste0(path, "/../images/scatterplot-radio-sales.png"))
grid.raster(radio.img)
```

```{r}
news.img <- readPNG(paste0(path, "/../images/scatterplot-news-sales.png"))
grid.raster(news.img)
```

The features collectively produce a model with the following residual plot:

```{r}
res.img <- readPNG(paste0(path, "/../images/residual-plot.png"))
grid.raster(res.img)
```

and normal qq plot:

```{r}
norm.img <- readPNG(paste0(path, "/../images/normal-qq-plot.png"))
grid.raster(norm.img)
```

### Conclusion:

Looking at the results for the simple linear models, we can see though the intercepts and slopes are significantly different from 0 (meaning there is a relationship between the advertising budges and 0), meaning all are better predictors of the response than guessing the mean. However the slopes themselves are not all very large; only `Radio` has a slope in the tenths, vs hundreths, decimal place. However, this isn't to say that the company should spent all its advertising budget on radio advertisements. `TV` has the greatest $R^2$-value, meaning it is best at explaining the variation in `Sales`, which can be clearly seen in the scatterplots. 

If we look at the multiple regression model, `Newspaper` is no longer has any significant impact on sales; its slope is now nearly 0 and p-value 0.86. This could be explained by the 0.35 correlation between `Newspaper` and `Radio`; markets with higher spending on news ads are spending more on radio ads too, and it's the radio ads that are boosting sales, not the news. `Radio` and `TV` have regression coefficients that don't vary much between the simple and multiple regression models, and together produce an $R^2$ value of 0.90, which is almost 0.30 better than either had individually. This demonstrates the importance of considering the influence of the relationships between features on our predictors, rather than trying to isolate each. There are also non-linear relationships to consider; the normal qq plot shows a bit a skew so our model is biased. The more hyperparameters involved, however, the more likely the model is prone to overfitting. Hence model selection and hyperparameter tuning always involves a bias-variance tradeoff. 

